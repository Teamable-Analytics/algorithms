import json
import os
import re
import shutil
import sys
from datetime import datetime
from functools import cached_property
from os import path
from typing import List, Dict, Any, TYPE_CHECKING

import git

from api.models.team_set import TeamSet
from api.models.team_set.serializer import TeamSetSerializer

if TYPE_CHECKING:
    from benchmarking.simulation.simulation import SimulationArtifact


FRAGMENT_FILE_NAME_PATTERN = r"(fragment_\d+)\.json"


class SimulationCache:
    """
    This class is used to cache the results of a simulation, which is a List[TeamSet] and List[float], where each element of the list is a different run and it's runtime.
    All TeamSets should have been generated by the same algorithm (with the same settings).
    It uses the cache_key to generate a file with the results of the simulation.
    """

    def __init__(self, cache_key: str) -> None:
        self.cache_key: str = cache_key
        self._data: Dict[str, Any] = {}
        """
        Dict in the form of:
        {
            "metadata": Dict[str, Any],  # Contains at least "timestamp" and "commit_hash"
            "team_sets": List[TeamSet],
            "runtimes": List[float],
        }
        """

    def is_fragmented(self) -> bool:
        maybe_cache_fragment_directory = SimulationCache.cache_key_parent_directory(
            self.cache_key
        )
        if not path.exists(maybe_cache_fragment_directory):
            return False
        if len(os.listdir(maybe_cache_fragment_directory)) < 1:
            print(f"[WARNING]: Empty directory @ {maybe_cache_fragment_directory}")
            return False

        for f in os.listdir(maybe_cache_fragment_directory):
            if re.match(FRAGMENT_FILE_NAME_PATTERN, f):
                return True

        return False

    def exists(self) -> bool:
        """
        Checks if the cache_key is in the cache.
        """
        if self.is_fragmented():
            return True
        return path.exists(self._get_file())

    def get_simulation_artifact(self) -> "SimulationArtifact":
        """
        Gets the simulation results from the cache.
        """
        self._load_data()

        return self._data["team_sets"], self._data["runtimes"]

    def get_metadata(self) -> Dict[str, Any]:
        """
        Gets the metadata associated with the simulation results from the cache.
        """
        self._load_data()

        return self._data["metadata"]

    def save(
        self,
        team_sets: List[TeamSet],
        runtimes: List[float],
        metadata: Dict[str, Any] = None,
    ) -> None:
        """
        Puts the simulation results into the cache.
        Overwrites any existing cache with the same cache_key.
        """

        if len(team_sets) != len(runtimes):
            raise ValueError(
                "The number of team_sets and runtimes must be the same. "
                f"Got {len(team_sets)} team_sets and {len(runtimes)} runtimes."
            )

        # Get metadata
        metadata = metadata or {}
        # Epoch time, num seconds since 1970, no timezone (utc)
        metadata["timestamp"] = (
            metadata.get("timestamp") or datetime.utcnow().timestamp()
        )
        # Get latest commit hash. This is so that we can track which commit generated the cache and go back to the code that generated it if needed. Would then be accessible at https://github.com/Teamable-Analytics/algorithms/commit/<commit_hash>
        current_commit_hash = git.Repo(
            search_parent_directories=True
        ).head.object.hexsha
        if (
            metadata.get("commit_hash")
            and metadata.get("commit_hash") != current_commit_hash
        ):
            print(
                "[WARNING]: The commit hash that you are writing to cache does not match the commit hash of your current branch",
                file=sys.stderr,
            )

        metadata["commit_hash"] = metadata.get("commit_hash") or current_commit_hash

        # Make dict that will be stored
        cached_data = {
            "metadata": metadata,
            "team_sets": team_sets,
            "runtimes": runtimes,
        }

        # Write to json file
        with open(self._get_file(), "w+") as file:
            json.dump(cached_data, file, cls=TeamSetSerializer)

        # Update the in memory cache now that the data has changed
        self._data = cached_data

    def add_run(self, team_set: TeamSet, runtime: float) -> None:
        """
        Adds a run to the cache.
        """
        if not self.exists():
            return self.save([team_set], [runtime])

        self._load_data()

        # Add run to data
        self._data["team_sets"].append(team_set)
        self._data["runtimes"].append(runtime)

        # Write to json file
        with open(self._get_file(), "w+") as f:
            json.dump(self._data, f, cls=TeamSetSerializer)

    def clear(self) -> None:
        """
        Clears the cache.
        """
        if self.exists():
            if self.is_fragmented():
                # Remove fragments and directory
                shutil.rmtree(
                    SimulationCache.cache_key_parent_directory(self.cache_key)
                )
            else:
                os.remove(self._get_file())
        self._data = {}

    def _get_file(self) -> str:
        """
        Gets the file associated with the cache_key.
        """

        # Get cache directory
        cache_dir = path.abspath(
            path.join(path.dirname(__file__), "..", "..", "simulation_cache")
        )

        # Create cache directory if it doesn't exist
        cache_key_dirs = path.normpath(self.cache_key).split(os.sep)
        filename = str(cache_key_dirs[-1]) + ".json"
        cache_key_dirs = cache_key_dirs[:-1]
        full_cache_dir = path.join(cache_dir, *cache_key_dirs)
        if not path.exists(full_cache_dir):
            os.makedirs(full_cache_dir)

        # Get file
        return path.join(full_cache_dir, filename)

    def _load_existing_data(self):
        if not self._data:
            if not self.exists() and not self.is_fragmented():
                raise FileNotFoundError("Existing cache data doesn't exist")

            # Load json data
            with open(self._get_file(), "r") as f:
                json_data = json.load(f)
            if not json_data:
                raise ValueError(
                    f'No json could be loaded from the "{self.cache_key}" cache'
                )

            # Init data to be the loaded json
            self._data = json_data

            # Convert json team sets to actual TeamSets
            self._data["team_sets"] = [
                TeamSetSerializer().decode(team_set)
                for team_set in json_data["team_sets"]
            ]

            self._data: Dict[str, Any] = json_data

    def _load_data(self) -> None:
        if not self._data:
            if not self.exists():
                raise FileNotFoundError("Cache doesn't exist")

            if self.is_fragmented():
                from benchmarking.caching.utils import combine

                combine(self.cache_key)

            # Load json data
            with open(self._get_file(), "r") as f:
                json_data = json.load(f)
            if not json_data:
                raise ValueError(
                    f'No json could be loaded from the "{self.cache_key}" cache'
                )

            # Init data to be the loaded json
            self._data = json_data

            # Convert json team sets to actual TeamSets
            self._data["team_sets"] = [
                TeamSetSerializer().decode(team_set)
                for team_set in json_data["team_sets"]
            ]

            self._data: Dict[str, Any] = json_data

    @staticmethod
    def get_fragment_cache_key(cache_key: str, fragment_id: int):
        return f"{cache_key}/fragment_{fragment_id}"

    @staticmethod
    def cache_key_parent_directory(cache_key: str):
        # Get cache directory
        cache_dir = path.abspath(
            path.join(path.dirname(__file__), "..", "..", "simulation_cache")
        )

        full_cache_dir = path.join(cache_dir, cache_key)

        return full_cache_dir

    @staticmethod
    def create_fragment_parent_dir(cache_key: str):
        full_cache_dir = SimulationCache.cache_key_parent_directory(cache_key)
        print(full_cache_dir)
        if not path.exists(full_cache_dir):
            os.makedirs(full_cache_dir)
